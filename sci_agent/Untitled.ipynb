{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8755e1a4-0d36-4e7a-af55-49c8dfdde81e",
   "metadata": {},
   "source": [
    "Let's tinker with collections a bit. \n",
    "1. build a collection\n",
    "2. add docs (Let's try to add these without ids)\n",
    "3. try to retrieve docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7a00cf0c-e210-47f6-b4bb-e6d1d1476be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Protocols']\n"
     ]
    }
   ],
   "source": [
    "from chromadb import PersistentClient\n",
    "DB_PATH = \"./.chroma_db\"\n",
    "db = PersistentClient(path=DB_PATH)\n",
    "print(db.list_collections())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c6f6221-856e-4ef2-9391-c04f2988c739",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch filenames\n",
    "from docx import Document\n",
    "import os\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from chromadb import PersistentClient\n",
    "\n",
    "\n",
    "DB_PATH = \"./.chroma_db\"\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "db = PersistentClient(path=DB_PATH)\n",
    "if 'Protocols' in db.list_collections():\n",
    "    db.delete_collection(name='Protocols')\n",
    "\n",
    "protocol_collection = db.get_or_create_collection(name='Protocols')\n",
    "\n",
    "\n",
    "def open_text_files(directory):\n",
    "    \n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:          \n",
    "            file_path = os.path.join(root, file)\n",
    "            file_name = os.path.splitext(file)[0]\n",
    "            if file.endswith(\".docx\"):\n",
    "                try:\n",
    "                    document = Document(file_path)\n",
    "                    text = []\n",
    "                    for paragraph in document.paragraphs:\n",
    "                        text.append(paragraph.text)\n",
    "                    content = \"\\n\".join(text)  \n",
    "                    #print(f\"File: {file_path}\\nContent:\\n{content}\\n---\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error opening {file_path}: {e}\")\n",
    "            else:\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        content = f.read()\n",
    "                        #print(f\"File: {file_path}\\nContent:\\n{content}\\n---\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error opening {file_path}: {e}\")\n",
    "            \n",
    "            chunks = text_splitter.split_text(content)\n",
    "            documents  = []\n",
    "            ids = []\n",
    "            metadatas = []\n",
    "            for i  in range(len(chunks)):\n",
    "                documents.append(chunks[i])\n",
    "                ids.append(f\"{file_name}_{i}\")\n",
    "                metadatas.append({\n",
    "                    'path': file_path,\n",
    "                    'chunk': i\n",
    "                })\n",
    "\n",
    "            \n",
    "            protocol_collection.add(documents=documents, ids=ids, metadatas=metadatas) \n",
    "                              \n",
    "           \n",
    "                \n",
    "\n",
    "open_text_files(\"protocols/\")\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dcad15f5-02cd-4421-92fc-7a936e0904bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = protocol_collection.query(\n",
    "    query_texts=[\"fetch me the western blot protocol\"], \n",
    "    n_results = 10\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "51c0ca42-cc08-4bd7-ab16-3fd4f6e8c0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protocols/archive\\western-blot-protocol.md\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def most_frequent(List):\n",
    "    occurence_count = Counter(List)\n",
    "    return occurence_count.most_common(1)[0][0]\n",
    "  \n",
    "print(most_frequent([x['path'] for x in results['metadatas'][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "db1089c1-ff3f-451b-95a7-70fd604af0dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['protocols/archive\\\\sub\\\\Protocol 1.docx',\n",
       " 'protocols/dna-extraction-protocol.md',\n",
       " 'protocols/southern-blot-protocol.md',\n",
       " 'protocols/southern-blot-protocol.md',\n",
       " 'protocols/dna-extraction-protocol.md',\n",
       " 'protocols/dna-extraction-protocol.md',\n",
       " 'protocols/southern-blot-protocol.md',\n",
       " 'protocols/protein-isolation-protocol.md',\n",
       " 'protocols/protein-isolation-protocol.md',\n",
       " 'protocols/northern-blot-protocol.md']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x['path'] for x in results['metadatas'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e3c06f-fc2d-470d-9bdd-cccdc26a7e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards:   0%|                                                                 | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from chromadb import PersistentClient, EmbeddingFunction, Embeddings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from typing import List\n",
    "\n",
    "MODEL_NAME = \"Alibaba-NLP/gte-Qwen2-1.5B-instruct\"\n",
    "DB_PATH = \"./.chroma_db\"\n",
    "\n",
    "class Protocol:\n",
    "    def __init__(self, path: str, text:str):\n",
    "        self.path = path\n",
    "        self.text  = text\n",
    "\n",
    "class CustomEmbeddingClass(EmbeddingFunction):\n",
    "    def __init__(self, model_name):\n",
    "        self.embedding_model = HuggingFaceEmbedding(model_name)\n",
    "\n",
    "    def __call__(self, input_texts: List[str])->Embeddings:\n",
    "        return [self.embedding_model.get_text_embedding(text) for text in input_texts]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "db = PersistentClient(path=DB_PATH)\n",
    "\n",
    "custom_embedding_function = CustomEmbeddingClass(MODEL_NAME)\n",
    "\n",
    "#collection = db.get_or_create_collection(name='SOPs', embedding_function=custom_embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d873717f-22bc-42a3-ba4b-58d841ce81d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def documents_to_json(folder_path):\n",
    "    \"\"\"\n",
    "    Reads all text files in a folder (including subfolders) and stores their content\n",
    "    in a JSON object along with their relative paths.\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing documents\n",
    "        \n",
    "    Returns:\n",
    "        dict: JSON-compatible dictionary with file paths and contents\n",
    "    \"\"\"\n",
    "    # Convert the folder path to a Path object\n",
    "    base_path = Path(folder_path)\n",
    "    \n",
    "    # Dictionary to store the results\n",
    "    documents = {}\n",
    "    \n",
    "    # Supported text file extensions\n",
    "    text_extensions = {'.txt', '.md', }\n",
    "    \n",
    "    try:\n",
    "        # Walk through all files in the folder and subfolders\n",
    "        for file_path in base_path.rglob('*'):\n",
    "            # Check if it's a file and has a text extension\n",
    "            if file_path.is_file() and file_path.suffix.lower() in text_extensions:\n",
    "                try:\n",
    "                    # Get the relative path from the base folder\n",
    "                    relative_path = str(file_path.relative_to(base_path))\n",
    "                    \n",
    "                    # Read the file content\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        content = file.read()\n",
    "                    \n",
    "                    # Store in dictionary\n",
    "                    documents[relative_path] = content\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading file {file_path}: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        # Convert to JSON string\n",
    "        json_output = json.dumps(documents, indent=2)\n",
    "        \n",
    "        # Save to a JSON file\n",
    "        output_path = base_path / 'documents.json'\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(json_output)\n",
    "            \n",
    "        return documents\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing folder: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your folder path\n",
    "    folder_path = \"./documents\"\n",
    "    result = documents_to_json(folder_path)\n",
    "    \n",
    "    if result:\n",
    "        print(f\"Successfully processed {len(result)} documents\")\n",
    "        print(f\"Output saved to {folder_path}/documents.json\")\n",
    "    else:\n",
    "        print(\"Failed to process documents\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
